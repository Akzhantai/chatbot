from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import torch

# Step 1: Load raw PDF(s)
DATA_PATH = "data/"


def load_pdf_files(data):
    loader = DirectoryLoader(data,
                             glob='*.pdf',
                             loader_cls=PyPDFLoader)

    documents = loader.load()
    return documents


documents = load_pdf_files(data=DATA_PATH)


# Step 2: Create Chunks
def create_chunks(extracted_data):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,
                                                   chunk_overlap=50)
    text_chunks = text_splitter.split_documents(extracted_data)
    return text_chunks


text_chunks = create_chunks(extracted_data=documents)


# Step 3: Create Vector Embeddings
def get_embedding_model():
    model_path = "models/sentence-transformers/all-MiniLM-L6-v2"
    embedding_model = HuggingFaceEmbeddings(model_name=model_path)
    return embedding_model


embedding_model = get_embedding_model()

# Step 4: Store embeddings in FAISS
DB_FAISS_PATH = "vectorstore/db_faiss"
db = FAISS.from_documents(text_chunks, embedding_model)
db.save_local(DB_FAISS_PATH)


# Step 5: Load DeepSeek Model Locally
def load_llm():
    model_name = "models/yandex"  # Replace with the actual path or model name
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto"
    )
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        temperature=0.5,
        max_length=512,
        device=0 if torch.cuda.is_available() else -1 # gpu
    )
    llm = HuggingFacePipeline(pipeline=pipe)
    return llm


# Step 6: Connect LLM with FAISS and Create chain
CUSTOM_PROMPT_TEMPLATE = """
Используй информацию из предоставленного контекста для ответа на вопрос пользователя.
Если ты не знаешь ответа, просто скажи, что не знаешь, не пытайся придумать ответ.
Не предоставляй информацию, которая не содержится в данном контексте.

Контекст: {context}
Вопрос: {question}

Начинай ответ сразу. Без лишних слов.
"""


def set_custom_prompt(custom_prompt_template):
    prompt = PromptTemplate(template=custom_prompt_template, input_variables=["context", "question"])
    return prompt


# Load Database
DB_FAISS_PATH = "vectorstore/db_faiss"
embedding_model = get_embedding_model()
db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)

# Create QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=load_llm(),  # Use the locally loaded DeepSeek model
    chain_type="stuff",
    retriever=db.as_retriever(search_kwargs={'k': 3}),
    return_source_documents=True,
    chain_type_kwargs={'prompt': set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}
)

# Now invoke with a single query
user_query = input("Введите ваш запрос: ")
response = qa_chain.invoke({'query': user_query})
print("ОТВЕТ: ", response["result"])
