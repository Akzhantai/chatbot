from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_community.vectorstores import FAISS
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import torch
import os
import re


os.environ["TORCH_USE_CUDA_DSA"] = "0"

# Step 1: Load raw PDF(s)
DATA_PATH = "data"


def load_pdf_files(data_path):
    try:
        loader = DirectoryLoader(data_path, glob='*.pdf', loader_cls=PyPDFLoader)
        documents = loader.load()
        return documents
    except Exception as e:
        print(f"Error loading PDF files: {e}")
        return []


documents = load_pdf_files(DATA_PATH)

# Step 2: Create Chunks
def create_chunks(extracted_data):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    text_chunks = text_splitter.split_documents(extracted_data)
    return text_chunks


text_chunks = create_chunks(documents)

# Step 3: Create Vector Embeddings
def get_embedding_model():
    model_path = "models/ all-MiniLM-L6-v2"  # Use a local path if SSL issues persist
    try:
        embedding_model = HuggingFaceEmbeddings(model_name=model_path)
        return embedding_model
    except Exception as e:
        print(f"Error loading embedding model: {e}")
        return None


embedding_model = get_embedding_model()

DB_FAISS_PATH = "vectorstore/db_faiss"

os.makedirs(DB_FAISS_PATH, exist_ok=True)

if text_chunks and embedding_model:
    try:
        db = FAISS.from_documents(text_chunks, embedding_model)
        db.save_local(DB_FAISS_PATH)
        print(f"FAISS database saved at: {DB_FAISS_PATH}")
    except Exception as e:
        print(f"Error saving FAISS database: {e}")
else:
    print("Error: Unable to create FAISS database due to missing chunks or embedding model.")
    db = None

def load_llm():
    model_name = "models/deepseek"  # Replace with the actual path or model name
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        )
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)

        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            temperature=0.7,
            max_length=512,
            truncation=True,
            repetition_penalty=1.2,
            device=device
        )
        llm = HuggingFacePipeline(pipeline=pipe)
        return llm
    except Exception as e:
        print(f"Error loading LLM: {e}")
        return None


# Step 6: Connect LLM with FAISS and Create chain
CUSTOM_PROMPT_TEMPLATE = """
Контекст: {context}
Вопрос: {question}

Ответь кратко и точно, используя информацию из контекста. Если информации недостаточно, скажи "Не знаю".
"""


def set_custom_prompt(custom_prompt_template):
    prompt = PromptTemplate(template=custom_prompt_template, input_variables=["context", "question"])
    return prompt


# Load Database
if os.path.exists(os.path.join(DB_FAISS_PATH, "index.faiss")):
    try:
        embedding_model = get_embedding_model()
        db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)
        print(f"FAISS database loaded from: {DB_FAISS_PATH}")
    except Exception as e:
        print(f"Error loading FAISS database: {e}")
        db = None
else:
    print(f"Error: FAISS database not found at {DB_FAISS_PATH}. Please ensure it has been created.")
    db = None

# Create QA chain
if db and load_llm():
    qa_chain = RetrievalQA.from_chain_type(
        llm=load_llm(),
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={'k': 5}),  # Increase the number of retrieved documents
        return_source_documents=True,
        chain_type_kwargs={'prompt': set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}
    )

    # Now invoke with a single query
    user_query = input("Введите ваш запрос: ")
    response = qa_chain.invoke({'query': user_query})

    # Print the retrieved context for debugging
    source_documents = response.get("source_documents", [])
    print("\nИСТОЧНИКИ:")
    for doc in source_documents:
        print(doc.page_content)

    # Post-process the response to extract only the answer
    response_text = response["result"].strip()  # Get the raw response
    response_text = response_text.split("Ответ:")[-1].strip()  # Extract only the answer part

    # Remove repetitive phrases
    response_text = re.sub(r'(\b\w+\b)( \1\b)+', r'\1', response_text)  # Remove repeated words
    response_text = re.sub(r'([.!?])\1+', r'\1', response_text)  # Remove repeated punctuation

    # Limit the response length
    response_text = response_text[:500]  # Truncate to 500 characters

    # Handle irrelevant queries gracefully
    if "Не знаю" in response_text or len(response_text.split()) < 5:
        response_text = "Извините, я не могу ответить на этот вопрос. Информация отсутствует в базе данных."

    print("ОТВЕТ:", response_text)
else:
    print("Error: Unable to create QA chain due to missing database or LLM.")
